{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders.parsers import GrobidParser\n",
    "from langchain_community.document_loaders.generic import GenericLoader\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_ollama import ChatOllama\n",
    "from langchain_chroma import Chroma\n",
    "from langchain import hub\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "from embedder import SPECTEREmbeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = GenericLoader.from_filesystem(\"./data/\", glob=\"*\", suffixes=[\".pdf\"], parser=GrobidParser(segment_sentences=False))\n",
    "docs = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'text': 'Language model pre-training has been shown to be effective for improving many natural language processing tasks (Dai and Le, 2015;Peters et al., 2018a;Radford et al., 2018;Howard and Ruder, 2018).These include sentence-level tasks such as natural language inference (Bowman et al., 2015;Williams et al., 2018) and paraphrasing (Dolan and Brockett, 2005), which aim to predict the relationships between sentences by analyzing them holistically, as well as token-level tasks such as named entity recognition and question answering, where models are required to produce fine-grained output at the token level (Tjong Kim Sang and De Meulder, 2003;Rajpurkar et al., 2016).', 'para': '1', 'bboxes': \"[[{'page': '1', 'x': '72.00', 'y': '579.72', 'h': '218.27', 'w': '9.46'}, {'page': '1', 'x': '72.00', 'y': '593.27', 'h': '218.27', 'w': '9.46'}, {'page': '1', 'x': '72.00', 'y': '606.82', 'h': '218.27', 'w': '9.46'}, {'page': '1', 'x': '72.00', 'y': '620.37', 'h': '218.27', 'w': '9.46'}, {'page': '1', 'x': '72.00', 'y': '633.92', 'h': '28.18', 'w': '9.46'}], [{'page': '1', 'x': '104.32', 'y': '633.92', 'h': '185.95', 'w': '9.46'}, {'page': '1', 'x': '72.00', 'y': '647.47', 'h': '218.27', 'w': '9.46'}, {'page': '1', 'x': '72.00', 'y': '661.02', 'h': '218.27', 'w': '9.46'}, {'page': '1', 'x': '72.00', 'y': '674.57', 'h': '218.27', 'w': '9.46'}, {'page': '1', 'x': '72.00', 'y': '688.12', 'h': '218.27', 'w': '9.46'}, {'page': '1', 'x': '72.00', 'y': '701.67', 'h': '218.27', 'w': '9.46'}, {'page': '1', 'x': '72.00', 'y': '715.22', 'h': '218.27', 'w': '9.46'}, {'page': '1', 'x': '72.00', 'y': '728.77', 'h': '218.27', 'w': '9.46'}, {'page': '1', 'x': '72.00', 'y': '742.32', 'h': '218.27', 'w': '9.46'}, {'page': '1', 'x': '72.00', 'y': '755.86', 'h': '186.63', 'w': '9.46'}]]\", 'pages': \"('1', '1')\", 'section_title': 'Introduction', 'section_number': '1', 'paper_title': 'BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding', 'file_path': 'data/1810.04805v2 (1).pdf'}, page_content='Language model pre-training has been shown to be effective for improving many natural language processing tasks (Dai and Le, 2015;Peters et al., 2018a;Radford et al., 2018;Howard and Ruder, 2018).These include sentence-level tasks such as natural language inference (Bowman et al., 2015;Williams et al., 2018) and paraphrasing (Dolan and Brockett, 2005), which aim to predict the relationships between sentences by analyzing them holistically, as well as token-level tasks such as named entity recognition and question answering, where models are required to produce fine-grained output at the token level (Tjong Kim Sang and De Meulder, 2003;Rajpurkar et al., 2016).'),\n",
       " Document(metadata={'text': 'There are two existing strategies for applying pre-trained language representations to downstream tasks: feature-based and fine-tuning.The feature-based approach, such as ELMo (Peters et al., 2018a), uses task-specific architectures that include the pre-trained representations as additional features.The fine-tuning approach, such as the Generative Pre-trained Transformer (OpenAI GPT) (Radford et al., 2018), introduces minimal task-specific parameters, and is trained on the downstream tasks by simply fine-tuning all pretrained parameters.The two approaches share the same objective function during pre-training, where they use unidirectional language models to learn general language representations.', 'para': '3', 'bboxes': \"[[{'page': '1', 'x': '318.19', 'y': '226.40', 'h': '207.36', 'w': '9.46'}, {'page': '1', 'x': '307.28', 'y': '239.95', 'h': '218.27', 'w': '9.46'}, {'page': '1', 'x': '307.28', 'y': '253.31', 'h': '195.26', 'w': '9.64'}], [{'page': '1', 'x': '508.58', 'y': '253.50', 'h': '16.96', 'w': '9.46'}, {'page': '1', 'x': '307.28', 'y': '267.05', 'h': '218.27', 'w': '9.46'}, {'page': '1', 'x': '307.28', 'y': '280.60', 'h': '218.27', 'w': '9.46'}, {'page': '1', 'x': '307.28', 'y': '294.14', 'h': '218.27', 'w': '9.46'}, {'page': '1', 'x': '307.28', 'y': '307.69', 'h': '65.14', 'w': '9.46'}], [{'page': '1', 'x': '376.70', 'y': '307.69', 'h': '148.84', 'w': '9.46'}, {'page': '1', 'x': '307.28', 'y': '321.24', 'h': '218.27', 'w': '9.46'}, {'page': '1', 'x': '307.28', 'y': '334.79', 'h': '218.27', 'w': '9.46'}, {'page': '1', 'x': '307.28', 'y': '348.34', 'h': '218.27', 'w': '9.46'}, {'page': '1', 'x': '307.28', 'y': '361.70', 'h': '218.27', 'w': '9.64'}, {'page': '1', 'x': '307.28', 'y': '375.44', 'h': '83.83', 'w': '9.46'}], [{'page': '1', 'x': '395.13', 'y': '375.44', 'h': '130.42', 'w': '9.46'}, {'page': '1', 'x': '307.28', 'y': '388.99', 'h': '218.27', 'w': '9.46'}, {'page': '1', 'x': '307.28', 'y': '402.54', 'h': '218.27', 'w': '9.46'}, {'page': '1', 'x': '307.28', 'y': '416.09', 'h': '145.71', 'w': '9.46'}]]\", 'pages': \"('1', '1')\", 'section_title': 'Introduction', 'section_number': '1', 'paper_title': 'BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding', 'file_path': 'data/1810.04805v2 (1).pdf'}, page_content='There are two existing strategies for applying pre-trained language representations to downstream tasks: feature-based and fine-tuning.The feature-based approach, such as ELMo (Peters et al., 2018a), uses task-specific architectures that include the pre-trained representations as additional features.The fine-tuning approach, such as the Generative Pre-trained Transformer (OpenAI GPT) (Radford et al., 2018), introduces minimal task-specific parameters, and is trained on the downstream tasks by simply fine-tuning all pretrained parameters.The two approaches share the same objective function during pre-training, where they use unidirectional language models to learn general language representations.'),\n",
       " Document(metadata={'text': 'We argue that current techniques restrict the power of the pre-trained representations, especially for the fine-tuning approaches.The major limitation is that standard language models are unidirectional, and this limits the choice of architectures that can be used during pre-training.For example, in OpenAI GPT, the authors use a left-toright architecture, where every token can only attend to previous tokens in the self-attention layers of the Transformer (Vaswani et al., 2017).Such restrictions are sub-optimal for sentence-level tasks, and could be very harmful when applying finetuning based approaches to token-level tasks such as question answering, where it is crucial to incorporate context from both directions.', 'para': '3', 'bboxes': \"[[{'page': '1', 'x': '318.19', 'y': '430.16', 'h': '207.36', 'w': '9.46'}, {'page': '1', 'x': '307.28', 'y': '443.71', 'h': '218.27', 'w': '9.46'}, {'page': '1', 'x': '307.28', 'y': '457.26', 'h': '169.55', 'w': '9.46'}], [{'page': '1', 'x': '486.72', 'y': '457.26', 'h': '38.83', 'w': '9.46'}, {'page': '1', 'x': '307.28', 'y': '470.81', 'h': '218.27', 'w': '9.46'}, {'page': '1', 'x': '307.28', 'y': '484.36', 'h': '218.27', 'w': '9.46'}, {'page': '1', 'x': '307.28', 'y': '497.91', 'h': '198.20', 'w': '9.46'}], [{'page': '1', 'x': '510.56', 'y': '497.91', 'h': '14.99', 'w': '9.46'}, {'page': '1', 'x': '307.28', 'y': '511.46', 'h': '218.27', 'w': '9.46'}, {'page': '1', 'x': '307.28', 'y': '525.00', 'h': '218.27', 'w': '9.46'}, {'page': '1', 'x': '307.28', 'y': '538.55', 'h': '218.27', 'w': '9.46'}, {'page': '1', 'x': '307.28', 'y': '552.10', 'h': '179.06', 'w': '9.46'}], [{'page': '1', 'x': '489.51', 'y': '552.10', 'h': '36.03', 'w': '9.46'}, {'page': '1', 'x': '307.28', 'y': '565.65', 'h': '218.27', 'w': '9.46'}, {'page': '1', 'x': '307.28', 'y': '579.20', 'h': '218.27', 'w': '9.46'}, {'page': '1', 'x': '307.28', 'y': '592.75', 'h': '218.27', 'w': '9.46'}, {'page': '1', 'x': '307.28', 'y': '606.30', 'h': '218.27', 'w': '9.46'}, {'page': '1', 'x': '307.28', 'y': '619.85', 'h': '156.48', 'w': '9.46'}]]\", 'pages': \"('1', '1')\", 'section_title': 'Introduction', 'section_number': '1', 'paper_title': 'BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding', 'file_path': 'data/1810.04805v2 (1).pdf'}, page_content='We argue that current techniques restrict the power of the pre-trained representations, especially for the fine-tuning approaches.The major limitation is that standard language models are unidirectional, and this limits the choice of architectures that can be used during pre-training.For example, in OpenAI GPT, the authors use a left-toright architecture, where every token can only attend to previous tokens in the self-attention layers of the Transformer (Vaswani et al., 2017).Such restrictions are sub-optimal for sentence-level tasks, and could be very harmful when applying finetuning based approaches to token-level tasks such as question answering, where it is crucial to incorporate context from both directions.'),\n",
       " Document(metadata={'text': 'In this paper, we improve the fine-tuning based approaches by proposing BERT: Bidirectional Encoder Representations from Transformers.BERT alleviates the previously mentioned unidirectionality constraint by using a \"masked language model\" (MLM) pre-training objective, inspired by the Cloze task (Taylor, 1953).The masked language model randomly masks some of the tokens from the input, and the objective is to predict the original vocabulary id of the masked word based only on its context.Unlike left-toright language model pre-training, the MLM objective enables the representation to fuse the left and the right context, which allows us to pretrain a deep bidirectional Transformer.In addition to the masked language model, we also use a \"next sentence prediction\" task that jointly pretrains text-pair representations.The contributions of our paper are as follows:', 'para': '5', 'bboxes': \"[[{'page': '1', 'x': '318.19', 'y': '633.92', 'h': '207.36', 'w': '9.46'}, {'page': '1', 'x': '307.28', 'y': '647.04', 'h': '218.27', 'w': '9.88'}, {'page': '1', 'x': '307.28', 'y': '660.59', 'h': '218.27', 'w': '9.88'}], [{'page': '1', 'x': '307.28', 'y': '674.57', 'h': '218.27', 'w': '9.46'}, {'page': '1', 'x': '307.28', 'y': '688.12', 'h': '218.27', 'w': '9.46'}, {'page': '1', 'x': '307.28', 'y': '701.67', 'h': '218.27', 'w': '9.46'}, {'page': '1', 'x': '307.28', 'y': '715.22', 'h': '189.63', 'w': '9.46'}], [{'page': '1', 'x': '508.58', 'y': '715.22', 'h': '16.96', 'w': '9.46'}, {'page': '1', 'x': '307.28', 'y': '728.77', 'h': '218.27', 'w': '9.46'}, {'page': '1', 'x': '307.28', 'y': '742.32', 'h': '218.27', 'w': '9.46'}, {'page': '1', 'x': '307.28', 'y': '755.86', 'h': '218.27', 'w': '9.46'}, {'page': '2', 'x': '72.00', 'y': '66.67', 'h': '145.08', 'w': '9.46'}], [{'page': '2', 'x': '225.86', 'y': '66.67', 'h': '64.41', 'w': '9.46'}, {'page': '2', 'x': '72.00', 'y': '80.22', 'h': '218.27', 'w': '9.46'}, {'page': '2', 'x': '72.00', 'y': '93.76', 'h': '218.27', 'w': '9.46'}, {'page': '2', 'x': '72.00', 'y': '107.31', 'h': '218.27', 'w': '9.46'}, {'page': '2', 'x': '72.00', 'y': '120.86', 'h': '174.21', 'w': '9.46'}], [{'page': '2', 'x': '254.42', 'y': '120.86', 'h': '35.85', 'w': '9.46'}, {'page': '2', 'x': '72.00', 'y': '134.41', 'h': '218.27', 'w': '9.46'}, {'page': '2', 'x': '72.00', 'y': '147.96', 'h': '218.27', 'w': '9.46'}, {'page': '2', 'x': '72.00', 'y': '161.51', 'h': '136.12', 'w': '9.46'}], [{'page': '2', 'x': '212.79', 'y': '161.51', 'h': '77.48', 'w': '9.46'}, {'page': '2', 'x': '72.00', 'y': '175.06', 'h': '119.39', 'w': '9.46'}]]\", 'pages': \"('1', '2')\", 'section_title': 'Introduction', 'section_number': '1', 'paper_title': 'BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding', 'file_path': 'data/1810.04805v2 (1).pdf'}, page_content='In this paper, we improve the fine-tuning based approaches by proposing BERT: Bidirectional Encoder Representations from Transformers.BERT alleviates the previously mentioned unidirectionality constraint by using a \"masked language model\" (MLM) pre-training objective, inspired by the Cloze task (Taylor, 1953).The masked language model randomly masks some of the tokens from the input, and the objective is to predict the original vocabulary id of the masked word based only on its context.Unlike left-toright language model pre-training, the MLM objective enables the representation to fuse the left and the right context, which allows us to pretrain a deep bidirectional Transformer.In addition to the masked language model, we also use a \"next sentence prediction\" task that jointly pretrains text-pair representations.The contributions of our paper are as follows:'),\n",
       " Document(metadata={'text': '• We demonstrate the importance of bidirectional pre-training for language representations.Unlike Radford et al. (2018), which uses unidirectional language models for pre-training, BERT uses masked language models to enable pretrained deep bidirectional representations.This is also in contrast to Peters et al. (2018a), which uses a shallow concatenation of independently trained left-to-right and right-to-left LMs.', 'para': '2', 'bboxes': \"[[{'page': '2', 'x': '74.11', 'y': '195.54', 'h': '216.15', 'w': '9.46'}, {'page': '2', 'x': '82.91', 'y': '209.09', 'h': '183.79', 'w': '9.46'}], [{'page': '2', 'x': '273.31', 'y': '209.09', 'h': '16.96', 'w': '9.46'}, {'page': '2', 'x': '82.91', 'y': '222.64', 'h': '207.36', 'w': '9.46'}, {'page': '2', 'x': '82.91', 'y': '236.19', 'h': '207.36', 'w': '9.46'}, {'page': '2', 'x': '82.91', 'y': '249.74', 'h': '207.36', 'w': '9.46'}, {'page': '2', 'x': '82.91', 'y': '263.29', 'h': '183.79', 'w': '9.46'}], [{'page': '2', 'x': '270.87', 'y': '263.29', 'h': '19.40', 'w': '9.46'}, {'page': '2', 'x': '82.91', 'y': '276.84', 'h': '207.36', 'w': '9.46'}, {'page': '2', 'x': '82.91', 'y': '290.39', 'h': '207.36', 'w': '9.46'}, {'page': '2', 'x': '82.91', 'y': '303.94', 'h': '182.09', 'w': '9.46'}]]\", 'pages': \"('2', '2')\", 'section_title': 'Introduction', 'section_number': '1', 'paper_title': 'BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding', 'file_path': 'data/1810.04805v2 (1).pdf'}, page_content='• We demonstrate the importance of bidirectional pre-training for language representations.Unlike Radford et al. (2018), which uses unidirectional language models for pre-training, BERT uses masked language models to enable pretrained deep bidirectional representations.This is also in contrast to Peters et al. (2018a), which uses a shallow concatenation of independently trained left-to-right and right-to-left LMs.'),\n",
       " Document(metadata={'text': '• We show that pre-trained representations reduce the need for many heavily-engineered taskspecific architectures.BERT is the first finetuning based representation model that achieves state-of-the-art performance on a large suite of sentence-level and token-level tasks, outperforming many task-specific architectures.', 'para': '1', 'bboxes': \"[[{'page': '2', 'x': '74.11', 'y': '325.64', 'h': '216.15', 'w': '9.46'}, {'page': '2', 'x': '82.91', 'y': '339.19', 'h': '207.36', 'w': '9.46'}, {'page': '2', 'x': '82.91', 'y': '352.74', 'h': '96.23', 'w': '9.46'}], [{'page': '2', 'x': '187.68', 'y': '352.74', 'h': '102.59', 'w': '9.46'}, {'page': '2', 'x': '82.91', 'y': '366.29', 'h': '207.36', 'w': '9.46'}, {'page': '2', 'x': '82.91', 'y': '379.84', 'h': '207.36', 'w': '9.46'}, {'page': '2', 'x': '82.91', 'y': '393.20', 'h': '207.36', 'w': '9.64'}, {'page': '2', 'x': '82.91', 'y': '406.94', 'h': '180.40', 'w': '9.46'}]]\", 'pages': \"('2', '2')\", 'section_title': 'Introduction', 'section_number': '1', 'paper_title': 'BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding', 'file_path': 'data/1810.04805v2 (1).pdf'}, page_content='• We show that pre-trained representations reduce the need for many heavily-engineered taskspecific architectures.BERT is the first finetuning based representation model that achieves state-of-the-art performance on a large suite of sentence-level and token-level tasks, outperforming many task-specific architectures.'),\n",
       " Document(metadata={'text': '• BERT advances the state of the art for eleven NLP tasks.The code and pre-trained models are available at https://github.com/ google-research/bert.', 'para': '1', 'bboxes': \"[[{'page': '2', 'x': '74.11', 'y': '428.64', 'h': '216.15', 'w': '9.46'}, {'page': '2', 'x': '82.91', 'y': '442.19', 'h': '50.50', 'w': '9.46'}], [{'page': '2', 'x': '144.67', 'y': '442.19', 'h': '145.60', 'w': '9.46'}, {'page': '2', 'x': '82.91', 'y': '455.74', 'h': '80.30', 'w': '9.46'}, {'page': '2', 'x': '210.66', 'y': '470.31', 'h': '6.39', 'w': '7.68'}]]\", 'pages': \"('2', '2')\", 'section_title': 'Introduction', 'section_number': '1', 'paper_title': 'BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding', 'file_path': 'data/1810.04805v2 (1).pdf'}, page_content='• BERT advances the state of the art for eleven NLP tasks.The code and pre-trained models are available at https://github.com/ google-research/bert.'),\n",
       " Document(metadata={'text': 'There is a long history of pre-training general language representations, and we briefly review the most widely-used approaches in this section.', 'para': '0', 'bboxes': \"[[{'page': '2', 'x': '72.00', 'y': '513.16', 'h': '218.27', 'w': '9.46'}, {'page': '2', 'x': '72.00', 'y': '526.71', 'h': '218.27', 'w': '9.46'}, {'page': '2', 'x': '72.00', 'y': '540.26', 'h': '195.13', 'w': '9.46'}]]\", 'pages': \"('2', '2')\", 'section_title': 'Related Work', 'section_number': '2', 'paper_title': 'BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding', 'file_path': 'data/1810.04805v2 (1).pdf'}, page_content='There is a long history of pre-training general language representations, and we briefly review the most widely-used approaches in this section.'),\n",
       " Document(metadata={'text': 'Learning widely applicable representations of words has been an active area of research for decades, including non-neural (Brown et al., 1992;Ando and Zhang, 2005;Blitzer et al., 2006) and neural (Mikolov et al., 2013;Pennington et al., 2014) methods.Pre-trained word embeddings are an integral part of modern NLP systems, offering significant improvements over embeddings learned from scratch (Turian et al., 2010).To pretrain word embedding vectors, left-to-right language modeling objectives have been used (Mnih and Hinton, 2009), as well as objectives to discriminate correct from incorrect words in left and right context (Mikolov et al., 2013).', 'para': '2', 'bboxes': \"[[{'page': '2', 'x': '72.00', 'y': '579.72', 'h': '218.27', 'w': '9.46'}, {'page': '2', 'x': '72.00', 'y': '593.27', 'h': '218.27', 'w': '9.46'}, {'page': '2', 'x': '72.00', 'y': '606.82', 'h': '218.27', 'w': '9.46'}, {'page': '2', 'x': '72.00', 'y': '620.37', 'h': '218.27', 'w': '9.46'}, {'page': '2', 'x': '72.00', 'y': '633.92', 'h': '218.27', 'w': '9.46'}, {'page': '2', 'x': '72.00', 'y': '647.47', 'h': '70.89', 'w': '9.46'}], [{'page': '2', 'x': '155.29', 'y': '647.47', 'h': '134.98', 'w': '9.46'}, {'page': '2', 'x': '72.00', 'y': '661.02', 'h': '218.27', 'w': '9.46'}, {'page': '2', 'x': '72.00', 'y': '674.57', 'h': '218.27', 'w': '9.46'}, {'page': '2', 'x': '72.00', 'y': '688.12', 'h': '182.88', 'w': '9.46'}], [{'page': '2', 'x': '258.61', 'y': '688.12', 'h': '31.66', 'w': '9.46'}, {'page': '2', 'x': '72.00', 'y': '701.67', 'h': '218.27', 'w': '9.46'}, {'page': '2', 'x': '72.00', 'y': '715.22', 'h': '218.27', 'w': '9.46'}, {'page': '2', 'x': '72.00', 'y': '728.77', 'h': '218.27', 'w': '9.46'}, {'page': '2', 'x': '72.00', 'y': '742.31', 'h': '218.27', 'w': '9.46'}, {'page': '2', 'x': '72.00', 'y': '755.86', 'h': '156.52', 'w': '9.46'}]]\", 'pages': \"('2', '2')\", 'section_title': 'Unsupervised Feature-based Approaches', 'section_number': '2.1', 'paper_title': 'BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding', 'file_path': 'data/1810.04805v2 (1).pdf'}, page_content='Learning widely applicable representations of words has been an active area of research for decades, including non-neural (Brown et al., 1992;Ando and Zhang, 2005;Blitzer et al., 2006) and neural (Mikolov et al., 2013;Pennington et al., 2014) methods.Pre-trained word embeddings are an integral part of modern NLP systems, offering significant improvements over embeddings learned from scratch (Turian et al., 2010).To pretrain word embedding vectors, left-to-right language modeling objectives have been used (Mnih and Hinton, 2009), as well as objectives to discriminate correct from incorrect words in left and right context (Mikolov et al., 2013).'),\n",
       " Document(metadata={'text': 'These approaches have been generalized to coarser granularities, such as sentence embeddings (Kiros et al., 2015;Logeswaran and Lee, 2018) or paragraph embeddings (Le and Mikolov, 2014).To train sentence representations, prior work has used objectives to rank candidate next sentences (Jernite et al., 2017;Logeswaran and Lee, 2018), left-to-right generation of next sentence words given a representation of the previous sentence (Kiros et al., 2015), or denoising autoencoder derived objectives (Hill et al., 2016).', 'para': '1', 'bboxes': \"[[{'page': '2', 'x': '318.19', 'y': '66.67', 'h': '207.36', 'w': '9.46'}, {'page': '2', 'x': '307.28', 'y': '80.21', 'h': '218.27', 'w': '9.46'}, {'page': '2', 'x': '307.28', 'y': '93.76', 'h': '218.27', 'w': '9.46'}, {'page': '2', 'x': '307.28', 'y': '107.31', 'h': '218.27', 'w': '9.46'}, {'page': '2', 'x': '307.28', 'y': '120.86', 'h': '28.18', 'w': '9.46'}], [{'page': '2', 'x': '345.88', 'y': '120.86', 'h': '179.66', 'w': '9.46'}, {'page': '2', 'x': '307.28', 'y': '134.41', 'h': '218.27', 'w': '9.46'}, {'page': '2', 'x': '307.28', 'y': '147.96', 'h': '218.27', 'w': '9.46'}, {'page': '2', 'x': '307.28', 'y': '161.51', 'h': '218.27', 'w': '9.46'}, {'page': '2', 'x': '307.28', 'y': '175.06', 'h': '218.27', 'w': '9.46'}, {'page': '2', 'x': '307.28', 'y': '188.61', 'h': '218.27', 'w': '9.46'}, {'page': '2', 'x': '307.28', 'y': '202.16', 'h': '196.96', 'w': '9.46'}]]\", 'pages': \"('2', '2')\", 'section_title': 'Unsupervised Feature-based Approaches', 'section_number': '2.1', 'paper_title': 'BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding', 'file_path': 'data/1810.04805v2 (1).pdf'}, page_content='These approaches have been generalized to coarser granularities, such as sentence embeddings (Kiros et al., 2015;Logeswaran and Lee, 2018) or paragraph embeddings (Le and Mikolov, 2014).To train sentence representations, prior work has used objectives to rank candidate next sentences (Jernite et al., 2017;Logeswaran and Lee, 2018), left-to-right generation of next sentence words given a representation of the previous sentence (Kiros et al., 2015), or denoising autoencoder derived objectives (Hill et al., 2016).')]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "efaa1c8bfdff489493834fa70c999b99",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 4 files:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/datum/miniforge3/envs/mlenv/lib/python3.12/site-packages/adapters/loading.py:165: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  state_dict = torch.load(weights_file, map_location=\"cpu\")\n"
     ]
    }
   ],
   "source": [
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=200)\n",
    "splits = text_splitter.split_documents(docs)\n",
    "# vectorstore = Chroma(persist_directory=\"./chroma_db\", collection_name=\"bert\").from_documents(documents=splits, embedding=SPECTEREmbeddings())\n",
    "vectorstore = Chroma(persist_directory=\"./chroma_db\", collection_name=\"bert\").from_documents(documents=docs, embedding=SPECTEREmbeddings())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/datum/miniforge3/envs/mlenv/lib/python3.12/site-packages/langsmith/client.py:323: LangSmithMissingAPIKeyWarning: API key must be provided when using hosted LangSmith API\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "retriever = vectorstore.as_retriever()\n",
    "prompt = hub.pull(\"rlm/rag-prompt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatOllama(model=\"llama3.1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_prompt = \"\"\"\n",
    "Use the following pieces of context to answer the question at the end.\n",
    "If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
    "If a query contains the keyword \"this\" it means the current set documents that you are working on.\n",
    "{context}\n",
    "Question: {question}\n",
    "Answer:\"\"\"\n",
    "\n",
    "custom_rag_prompt = PromptTemplate.from_template(custom_prompt)\n",
    "\n",
    "rag_chain = (\n",
    "    {\"context\": retriever | format_docs, \"question\": RunnablePassthrough()}\n",
    "    | custom_rag_prompt\n",
    "    | llm \n",
    "    | StrOutputParser()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7bb855356e474148abe491876b5cada8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 4 files:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/datum/miniforge3/envs/mlenv/lib/python3.12/site-packages/adapters/loading.py:165: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  state_dict = torch.load(weights_file, map_location=\"cpu\")\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"I don't know. The provided text only mentions a comparison between BERT, ELMo, and OpenAI GPT, but it does not provide any information about how the BERT model differs specifically from the GPT model.\""
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rag_chain.invoke(\"How is the BERT model different from the GPT model?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
